name: Database Backup to R2

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

env:
  SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
  AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
  R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
  R2_BUCKET: ${{ secrets.R2_BUCKET }}

jobs:
  backup:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Get timestamp and backup metadata
        id: meta
        run: |
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H-%M-%SZ")
          HOUR=$(date -u +"%H")
          DAY_OF_WEEK=$(date -u +"%u")
          DATE=$(date -u +"%Y-%m-%d")

          echo "timestamp=$TIMESTAMP" >> "$GITHUB_OUTPUT"
          echo "hour=$HOUR" >> "$GITHUB_OUTPUT"
          echo "day_of_week=$DAY_OF_WEEK" >> "$GITHUB_OUTPUT"
          echo "date=$DATE" >> "$GITHUB_OUTPUT"

      - name: Install PostgreSQL 17 client
        run: |
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          curl -fsSL https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/postgresql.gpg
          sudo apt-get update -qq
          sudo apt-get install -y -qq postgresql-client-17
          echo "/usr/lib/postgresql/17/bin" >> "$GITHUB_PATH"
          /usr/lib/postgresql/17/bin/pg_dump --version

      - name: Create backup directory
        run: mkdir -p backups

      - name: Diagnose accessible schemas
        run: |
          echo "=== Schemas accessible ==="
          psql "$SUPABASE_DB_URL" -t -A -c "SELECT schema_name FROM information_schema.schemata ORDER BY schema_name;"
          echo ""
          echo "=== Tables per schema ==="
          psql "$SUPABASE_DB_URL" -t -A -c "SELECT schemaname, count(*) FROM pg_tables WHERE schemaname NOT IN ('pg_catalog','information_schema') GROUP BY schemaname ORDER BY schemaname;"
          echo ""
          echo "=== Total row counts (public) ==="
          psql "$SUPABASE_DB_URL" -t -A -c "SELECT relname, n_live_tup FROM pg_stat_user_tables WHERE schemaname='public' ORDER BY n_live_tup DESC LIMIT 20;"
          echo ""
          echo "=== Database size ==="
          psql "$SUPABASE_DB_URL" -t -A -c "SELECT pg_size_pretty(pg_database_size(current_database()));"

      - name: Dump full database (schema + data + functions + triggers)
        run: |
          pg_dump "$SUPABASE_DB_URL" \
            --no-owner \
            --no-privileges \
            --format=custom \
            --compress=9 \
            -N _analytics \
            -N _realtime \
            -N _timescaledb_cache \
            -N _timescaledb_catalog \
            -N _timescaledb_config \
            -N _timescaledb_internal \
            -N pgsodium \
            -N pgsodium_masks \
            -N pgtle \
            -N supabase_functions \
            -N supabase_migrations \
            -N graphql \
            -N graphql_public \
            -N net \
            -N tiger \
            -N tiger_data \
            -N topology \
            -f backups/full.dump
          echo "Full dump size: $(du -h backups/full.dump | cut -f1)"

      - name: Dump schema only (readable SQL)
        run: |
          pg_dump "$SUPABASE_DB_URL" \
            --schema-only \
            --no-owner \
            --no-privileges \
            --format=plain \
            -N _analytics \
            -N _realtime \
            -N _timescaledb_cache \
            -N _timescaledb_catalog \
            -N _timescaledb_config \
            -N _timescaledb_internal \
            -N pgsodium \
            -N pgsodium_masks \
            -N pgtle \
            -N supabase_functions \
            -N supabase_migrations \
            -N graphql \
            -N graphql_public \
            -N net \
            -N tiger \
            -N tiger_data \
            -N topology \
            -f backups/schema.sql
          echo "Schema dump size: $(du -h backups/schema.sql | cut -f1)"

      - name: Create compressed archive
        id: archive
        env:
          TIMESTAMP: ${{ steps.meta.outputs.timestamp }}
        run: |
          ARCHIVE="formline-backup-${TIMESTAMP}.tar.gz"
          tar -czf "backups/${ARCHIVE}" -C backups full.dump schema.sql
          echo "name=${ARCHIVE}" >> "$GITHUB_OUTPUT"
          echo "Archive size: $(du -h "backups/${ARCHIVE}" | cut -f1)"

      - name: Upload hourly backup to R2
        env:
          ARCHIVE: ${{ steps.archive.outputs.name }}
          TIMESTAMP: ${{ steps.meta.outputs.timestamp }}
        run: |
          aws s3 cp "backups/${ARCHIVE}" \
            "s3://${R2_BUCKET}/hourly/formline-backup-${TIMESTAMP}.tar.gz" \
            --endpoint-url "$R2_ENDPOINT"
          echo "Uploaded hourly backup"

      - name: Upload daily backup to R2 (at midnight UTC)
        if: steps.meta.outputs.hour == '00' || github.event_name == 'workflow_dispatch'
        env:
          ARCHIVE: ${{ steps.archive.outputs.name }}
          DATE: ${{ steps.meta.outputs.date }}
        run: |
          aws s3 cp "backups/${ARCHIVE}" \
            "s3://${R2_BUCKET}/daily/formline-backup-${DATE}.tar.gz" \
            --endpoint-url "$R2_ENDPOINT"
          echo "Uploaded daily backup"

      - name: Upload weekly backup to R2 (Sundays at midnight UTC)
        if: (steps.meta.outputs.hour == '00' && steps.meta.outputs.day_of_week == '7') || github.event_name == 'workflow_dispatch'
        env:
          ARCHIVE: ${{ steps.archive.outputs.name }}
          DATE: ${{ steps.meta.outputs.date }}
        run: |
          aws s3 cp "backups/${ARCHIVE}" \
            "s3://${R2_BUCKET}/weekly/formline-backup-${DATE}.tar.gz" \
            --endpoint-url "$R2_ENDPOINT"
          echo "Uploaded weekly backup"

      - name: Verify upload
        run: |
          echo "=== Recent hourly backups ==="
          aws s3 ls "s3://${R2_BUCKET}/hourly/" --endpoint-url "$R2_ENDPOINT" | tail -5
          echo ""
          echo "=== Recent daily backups ==="
          aws s3 ls "s3://${R2_BUCKET}/daily/" --endpoint-url "$R2_ENDPOINT" | tail -5
          echo ""
          echo "=== Recent weekly backups ==="
          aws s3 ls "s3://${R2_BUCKET}/weekly/" --endpoint-url "$R2_ENDPOINT" | tail -5

  cleanup:
    runs-on: ubuntu-latest
    needs: backup
    timeout-minutes: 10
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
      R2_BUCKET: ${{ secrets.R2_BUCKET }}
    steps:
      - name: Get retention cutoff dates
        id: cutoff
        run: |
          HOURLY_CUTOFF=$(date -u -d '25 hours ago' +"%Y-%m-%dT%H")
          DAILY_CUTOFF=$(date -u -d '31 days ago' +"%Y-%m-%d")
          WEEKLY_CUTOFF=$(date -u -d '91 days ago' +"%Y-%m-%d")
          echo "hourly=$HOURLY_CUTOFF" >> "$GITHUB_OUTPUT"
          echo "daily=$DAILY_CUTOFF" >> "$GITHUB_OUTPUT"
          echo "weekly=$WEEKLY_CUTOFF" >> "$GITHUB_OUTPUT"

      - name: Cleanup old hourly backups (keep last 25)
        env:
          CUTOFF: ${{ steps.cutoff.outputs.hourly }}
        run: |
          aws s3 ls "s3://${R2_BUCKET}/hourly/" --endpoint-url "$R2_ENDPOINT" | while read -r line; do
            FILE=$(echo "$line" | awk '{print $4}')
            if [ -n "$FILE" ]; then
              FILE_TS=$(echo "$FILE" | grep -oP '\d{4}-\d{2}-\d{2}T\d{2}' || true)
              if [ -n "$FILE_TS" ] && [[ "$FILE_TS" < "$CUTOFF" ]]; then
                echo "Deleting old hourly: $FILE"
                aws s3 rm "s3://${R2_BUCKET}/hourly/$FILE" --endpoint-url "$R2_ENDPOINT"
              fi
            fi
          done

      - name: Cleanup old daily backups (keep last 31)
        env:
          CUTOFF: ${{ steps.cutoff.outputs.daily }}
        run: |
          aws s3 ls "s3://${R2_BUCKET}/daily/" --endpoint-url "$R2_ENDPOINT" | while read -r line; do
            FILE=$(echo "$line" | awk '{print $4}')
            if [ -n "$FILE" ]; then
              FILE_DATE=$(echo "$FILE" | grep -oP '\d{4}-\d{2}-\d{2}' || true)
              if [ -n "$FILE_DATE" ] && [[ "$FILE_DATE" < "$CUTOFF" ]]; then
                echo "Deleting old daily: $FILE"
                aws s3 rm "s3://${R2_BUCKET}/daily/$FILE" --endpoint-url "$R2_ENDPOINT"
              fi
            fi
          done

      - name: Cleanup old weekly backups (keep last 91 days)
        env:
          CUTOFF: ${{ steps.cutoff.outputs.weekly }}
        run: |
          aws s3 ls "s3://${R2_BUCKET}/weekly/" --endpoint-url "$R2_ENDPOINT" | while read -r line; do
            FILE=$(echo "$line" | awk '{print $4}')
            if [ -n "$FILE" ]; then
              FILE_DATE=$(echo "$FILE" | grep -oP '\d{4}-\d{2}-\d{2}' || true)
              if [ -n "$FILE_DATE" ] && [[ "$FILE_DATE" < "$CUTOFF" ]]; then
                echo "Deleting old weekly: $FILE"
                aws s3 rm "s3://${R2_BUCKET}/weekly/$FILE" --endpoint-url "$R2_ENDPOINT"
              fi
            fi
          done

      - name: Show backup inventory
        run: |
          echo "=== Backup Inventory ==="
          HOURLY_COUNT=$(aws s3 ls "s3://${R2_BUCKET}/hourly/" --endpoint-url "$R2_ENDPOINT" | wc -l)
          DAILY_COUNT=$(aws s3 ls "s3://${R2_BUCKET}/daily/" --endpoint-url "$R2_ENDPOINT" | wc -l)
          WEEKLY_COUNT=$(aws s3 ls "s3://${R2_BUCKET}/weekly/" --endpoint-url "$R2_ENDPOINT" | wc -l)
          echo "Hourly: $HOURLY_COUNT files"
          echo "Daily:  $DAILY_COUNT files"
          echo "Weekly: $WEEKLY_COUNT files"
